{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the scipy minimization routine(s).\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background ##\n",
    "\n",
    "In 1801 a young Gauss became an overnight sensation when he rediscovered the minor planet [Ceres](https://en.wikipedia.org/wiki/Ceres_(dwarf_planet)), which had been \"lost\" by its discoverer (Piazzi) earlier that year.  You can read more of the history and his methodology [here](https://math.berkeley.edu/~mgu/MA221/Ceres_Presentation.pdf) and [here](http://sites.math.rutgers.edu/~cherlin/History/Papers1999/weiss.html).  In brief he was clever, he calculated by hand (for over 100 hours), and he invented the method of least squares.  We're not as clever and we'll use a computer, but we'll follow his lead and use the method of least squares too.  Hopefully this will take less than 100 hours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbit determination through minimization ###\n",
    "\n",
    "In principle we could write out the ODEs for the motion of Ceres in the potential of the Sun, and hence get a prediction for the position as a function of time (given some initial conditions) by using e.g. ```solve_ivp```.  By comparing those predictions to the observations finding the orbit of Ceres now becomes a minimization problem, in 6D (initial position and velocity components).  This is basically what Gauss did, by being very clever about it.\n",
    "                                                                                            While we could do this in gory detail, we want to simplify this for the purposes of this problem set ... so let's try a \"toy\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified problem ###\n",
    "\n",
    "We want to simplify this problem as much as possible.  As a first step let's destroy the Earth (harsh, but in a good cause!) and move to the Sun (i.e. the origin).  Secondly let's decide that we will only consider observations close to the $x-y$ plane and near $y=0$, so that the sky looks flat to us and we can avoid arctangents and things and just look in the plane of the sky.  We can always rotate our coordinates so any given point lies along $(1,0,0)$.\n",
    "\n",
    "Further, suppose Ceres is distant from the Sun, so its orbital time is long, and suppose that the observations are taken over a short time period.  Now we can Taylor series expand the path of Ceres around the central observation:\n",
    "$$\n",
    "  \\vec{x}(t) \\simeq \\vec{x}_0 + (t-t_0)\\left.\\frac{d\\vec{x}}{dt}\\right|_0\n",
    "    + \\frac{1}{2}(t-t_0)^2 \\left.\\frac{d^2\\vec{x}}{dt^2}\\right|_0\n",
    "    = \\vec{x}_0 + (t-t_0)\\vec{v}_0 - \\frac{k\\hat{x}_0}{2r_0^2}(t-t_0)^2\n",
    "$$\n",
    "where $r_0^2=|\\vec{x}_0|^2$ and $k$ is a constant (which is equal to $4\\pi$ if we measure times in years, masses in solar masses and distances in AU).  For fixed observations $\\vec{x}(t_i)$ is a function of $\\vec{x}_0$ and $\\vec{v}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization ##\n",
    "\n",
    "### Minimization without gradient information ###\n",
    "\n",
    "When deciding how to minimize this function numerically we first need to decide whether or not we have gradient information.  If we do, then the minimization should be much more efficient because we have a guess as to which direction to step to find the minimum (i.e. opposite the gradient direction).  This becomes a bigger and bigger deal the higher the dimension of the space.  Speaking roughly, it take $\\mathcal{O}(N^2_{\\rm par})$ steps to find the minimum without gradient information and  $\\mathcal{O}(N_{\\rm par})$ with it.\n",
    "\n",
    "Without gradient information some common (though non-exhaustive) choices are:\n",
    "\n",
    "* Random flailing around.\n",
    "* Pattern search.\n",
    "* Simplex.\n",
    "* Powell.\n",
    "* Levenburg-Marquardt.\n",
    "* Trust regions and model functions.\n",
    "* Accelerated particle swarm optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization with gradient information ###\n",
    "\n",
    "With gradient information things become easier.  The way we would probably proceed with our problem is to use an \n",
    "[automatic differentation](https://en.wikipedia.org/wiki/Automatic_differentiation) library (or language) to generate the gradients of our function for us.  Common tools include AutoDiff, Stan or JAX or languages such as Julia.  The basic idea behind such tools is that since the computer is performing some sequence of basic operations to compute $\\chi^2$, and since we know (a) how to compute the derivative of operations like $x^2$ or $\\arctan x$ and (b) we know how to use the chain rule to \"compose\" multiple operations it's an algorithmic matter to compute the derivatives automagically.\n",
    "\n",
    "To avoid introducing more libraries and tools, let's do the derivatives by hand since our problem isn't all that complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our \"observations\" ##\n",
    "\n",
    "We will imagine we are given $\\ge 3$ observations of the projected position on the sky, at times including $t<t_0$, $t=t_0$ and $t>t_0$.  By choice of time origin (epoch) we can always make $t_0=0$.  Note we need at least 3 positions, because we need to solve for $\\vec{x}_0$ and $\\vec{v}_0$ which is 6 numbers and we get 2 constraints per observation.  We orient our coordinates such that $\\vec{x}_0\\propto\\widehat{x}$, and then our data are the $y$ and $z$ positions at the earlier and later times, converted to angles (in the small-angle approximation) by dividing by $|\\vec{x}|$.  This gives 4 data points and we need to solve for the length of $\\vec{x}_0$ and for $\\vec{v}_0$, i.e. 4 unknowns.  We have $r_0$ as the length of $\\vec{x}_0$ so our free parameters are $(r_0,v_{x0},v_{y0},v_{z0})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some code to return $\\vec{x}$ given $t$ and the parameters $\\vec{\\alpha}=(r_0,v_{x0},v_{y0},v_{z0})$ assuming the quadratic model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tt,alpha):\n",
    "    \"\"\"Returns the predicted \\vec{x}.  Note r(t)=|x(t)|.\"\"\"\n",
    "    kSol= 4*np.pi**2                # GM in Msun, AU, yr units. and astronomical units.\n",
    "    ...\n",
    "    xt  = ...\n",
    "    return(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some data, given some \"true\" parameters, $\\alpha$, and some observation times.  We'll call $\\theta=y/r$ and $\\phi=z/r$ just to have some names for them, though note these aren't the usual polar angles.  We only actually need two \"additional\" observations beyond the one at $t_0=0$ but adding a few extra helps constrain the problem, so why don't we choose 3 \"extra\" observations (4 total).\n",
    "\n",
    "Also, we won't add any \"noise\" to these observations so we expect that we will recover the \"true\" initial conditions at $\\chi^2=0$ and this should be independent of what we choose for our error bar in $\\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_true= np.array([2.0,0.0,1.0,0.5])\n",
    "obstime   = np.array([-0.50,0.1,0.75])\n",
    "tobs,pobs = np.zeros_like(obstime),np.zeros_like(obstime)\n",
    "for i,tt in enumerate(obstime):\n",
    "    pred    = prediction(tt,alpha_true)\n",
    "    rr      = np.sqrt(np.sum(pred**2))\n",
    "    tobs[i] = pred[1]/rr\n",
    "    pobs[i] = pred[2]/rr\n",
    "#\n",
    "print(\"time=\",obstime)\n",
    "print(\"tobs=\",tobs)\n",
    "print(\"pobs=\",pobs)\n",
    "# Set the \"observation accuracy\":\n",
    "sigma = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $\\chi^2$ is\n",
    "$$\n",
    "  \\chi^2 = \\frac{1}{\\sigma^2} \\sum_{i\\ne 0}\n",
    "  \\left(\\frac{y(t_i)}{r(t_i)}-\\theta_i^{\\rm obs}\\right)^2 +\n",
    "  \\left(\\frac{z(t_i)}{r(t_i)}-\\phi_i^{\\rm obs}\\right)^2\n",
    "$$\n",
    "with\n",
    "$$\n",
    "  \\vec{x}(t) = r_0\\widehat{x} + t\\vec{v}_0 - \\frac{k\\widehat{x}}{2r_0^2}t^2\n",
    "  = \\left( \\begin{array}{c} \\rho + tv_{x0} \\\\ tv_{y0} \\\\ tv_{z0} \\end{array}\\right)\n",
    "  \\quad , \\quad\n",
    "  \\left|\\vec{x}(t)\\right| = t\\sqrt{ (\\rho/t+v_{x0})^2+v_{y0}^2+v_{z0}^2 }\n",
    "$$\n",
    "where I've written $\\rho=r_0-(k/2r_0^2)t^2$ for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to return $\\chi^2$ for a 4-dimensional vector $\\alpha$ with the parameters ordered as $r_0$, $v_{x0}$, $v_{y0}$ and $v_{z0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(alpha):\n",
    "    \"\"\"Computes chi2 at point alpha.\"\"\"\n",
    "    c2 = 0.0\n",
    "    # There are contributions to chi2 from each time:\n",
    "    for i,tt in enumerate(obstime):\n",
    "        pred  = prediction(tt,alpha)\n",
    "        rr    = np.sqrt(np.sum(pred**2))\n",
    "        yor   = pred[1]/rr\n",
    "        zor   = pred[2]/rr\n",
    "        c2   += (yor-tobs[i])**2 + (zor-pobs[i])**2\n",
    "    return(c2/sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of our $\\chi^2$ value can be built up in pieces using the chain rule and the simple building blocks that we have.  If I call the parameter $\\alpha$, (i.e. $\\alpha$ is one of $r_0$, $v_{x0}$, etc) then\n",
    "$$\n",
    "  \\frac{\\partial\\chi^2}{\\partial\\alpha} =\n",
    "  \\frac{2}{\\sigma^2}\\sum_{i\\ne 0} \n",
    "  \\left(\\frac{y(t_i)}{r(t_i)}-\\theta_i^{\\rm obs}\\right)\\frac{\\partial y/r}{\\partial\\alpha} +\n",
    "  \\left(\\frac{z(t_i)}{r(t_i)}-\\phi_i^{\\rm obs}\\right)\\frac{\\partial z/r}{\\partial\\alpha}\n",
    "$$\n",
    "and for example\n",
    "$$\n",
    "  \\frac{\\partial y/r}{\\partial\\alpha} = \\frac{1}{r}\\frac{\\partial y}{\\partial\\alpha}\n",
    "  -\\frac{y}{r^2}\\frac{\\partial r}{\\partial\\alpha}\n",
    "$$\n",
    "with the other terms being similar.  Now $\\partial y/\\partial\\alpha$ is very simple, since $y=tv_{y0}$.  This means $\\partial y/\\partial\\alpha=0$ unless $\\alpha=v_{y0}$ in which case it is $t$.  The slightly more complicated case is $\\partial r/\\partial\\alpha$, but this is also reasonably straightforward:\n",
    "$$\n",
    "  \\frac{\\partial r}{\\partial\\alpha} = \\frac{t^2}{2|\\vec{x}(t)|}\n",
    "  \\frac{\\partial\\left[(\\rho/t+v_{x0})^2+v_{y0}^2+v_{z0}^2\\right]}{\\partial\\alpha}\n",
    "$$\n",
    "and the final derivative is very easy for all but $\\alpha=r_0$.\n",
    "\n",
    "Each step is straightforward, if slightly tedious, and we can build this up in code by computing each of the building blocks.  The code is easier to read, to understand and to debug if you try to break it up in pieces like we have above rather than attempting to grind out all of the formulae and type in one huge code block.\n",
    "\n",
    "Finish off the routine below that computes $\\partial\\chi^2/\\partial\\alpha_i$ for parameters $\\alpha_i$ ordered as $\\vec{\\alpha}=(r_0,v_{x0},v_{y0},v_{z0})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivs(alpha):\n",
    "    \"\"\"Computes the derivatives of chi2 with respect to the 4 parameters, at the point alpha.\"\"\"\n",
    "    # Make some storage for the derivatives and define sigma.\n",
    "    dd  = np.zeros(4) # The derivatives go here.\n",
    "    kSol= 4*np.pi**2  # GM in Msun, AU, yr units. and astronomical units.\n",
    "    # There are contributions to dchi2 from each time:\n",
    "    for i,tt in enumerate(obstime):\n",
    "        pred  = prediction(tt,alpha)\n",
    "        rr    = np.sqrt(np.sum(pred**2))\n",
    "        rho   = alpha[0] - kSol/2/alpha[0]**2*tt**2\n",
    "        yor   = pred[1]/rr\n",
    "        zor   = pred[2]/rr\n",
    "        # Start with v_y0 and v_z0, i.e. the last two parameters.\n",
    "        # First do the v_y0 derivative.\n",
    "        dy    = tt\n",
    "        dz    = 0\n",
    "        dr    = tt**2/rr * alpha[2]\n",
    "        dyor  = 1/rr*dy - pred[1]/rr**2*dr\n",
    "        dzor  = 1/rr*dz - pred[2]/rr**2*dr\n",
    "        dd[2]+= (yor-tobs[i])*dyor + (zor-pobs[i])*dzor\n",
    "        # and then the v_z0 derivative.\n",
    "        dy    = 0\n",
    "        dz    = tt\n",
    "        dr    = tt**2/rr * alpha[3]\n",
    "        dyor  = 1/rr*dy - pred[1]/rr**2*dr\n",
    "        dzor  = 1/rr*dz - pred[2]/rr**2*dr\n",
    "        dd[3]+= (yor-tobs[i])*dyor + (zor-pobs[i])*dzor\n",
    "        # Now the v_x0 derivative, which is only slightly more complicated.\n",
    "        ...\n",
    "        dd[1]+= (yor-tobs[i])*dyor + (zor-pobs[i])*dzor\n",
    "        # and finally the r_0 derivative, which is the most complicated.\n",
    "        ...\n",
    "        dd[0]+= (yor-tobs[i])*dyor + (zor-pobs[i])*dzor\n",
    "    # Finally multiply by the overall prefactor and return the answer.\n",
    "    dd *= 2/sigma**2\n",
    "    return(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing minimizations ###\n",
    "\n",
    "Now let us compare minimizations with and without gradients.  We'll start with the no-gradient version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the minimizer and compare to the \"true\" value.  How many function evaluations (```nfev```) does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No gradient information -- Powell.\n",
    "init_guess = np.array([1.9,0.0,1.1,0.4])\n",
    "res = minimize(chi2,init_guess,method='powell')\n",
    "#\n",
    "for i in range(init_guess.size):\n",
    "    print(\"alpha[{:d}]={:8.4f} vs. truth {:8.4f}\".\\\n",
    "          format(i,res.x[i],alpha_true[i]))\n",
    "print(\"Taking {:d} function evaluations.\".format(res.nfev))\n",
    "print(\"Final chi2={:e}\\n\".format(chi2(res.x)))\n",
    "# Compare to the data\n",
    "for i,tt in enumerate(obstime):\n",
    "    pred    = prediction(tt,res.x)\n",
    "    rr      = np.sqrt(np.sum(pred**2))\n",
    "    print(\"tobs[{:d}]={:10.5f} vs predicted {:10.5f}\".format(i,tobs[i],pred[1]/rr))\n",
    "    print(\"pobs[{:d}]={:10.5f} vs predicted {:10.5f}\".format(i,pobs[i],pred[2]/rr))\n",
    "# and we can actually look at the underlying 3D positions as well.\n",
    "print(\"\\n\")\n",
    "for i,tt in enumerate(obstime):\n",
    "    truth   = prediction(tt,alpha_true)\n",
    "    pred    = prediction(tt,res.x)\n",
    "    #\n",
    "    truth   = [\"{:10.5f}\".format(x) for x in truth]\n",
    "    pred    = [\"{:10.5f}\".format(x) for x in pred]\n",
    "    #\n",
    "    outstr  = \"x({:6.2f})=\".format(tt)\n",
    "    for itm in truth: outstr += itm\n",
    "    outstr += \" vs predicted \"\n",
    "    for itm in pred: outstr += itm\n",
    "    print(outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try this with a minimizer than can use gradient information.  We'll start from exactly the same place with the same data.  You can try different minimizers, below I'll use conjugate gradient as representative of this class of minimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient information -- conjugate-gradient (cg).\n",
    "res = minimize(chi2,init_guess,jac=derivs,method='cg')\n",
    "#\n",
    "for i in range(init_guess.size):\n",
    "    print(\"alpha[{:d}]={:8.4f} vs. truth {:8.4f}\".\\\n",
    "          format(i,res.x[i],alpha_true[i]))\n",
    "print(\"Taking {:d} function evaluations.\".format(res.nfev))\n",
    "print(\"Final chi2={:e}\\n\".format(chi2(res.x)))\n",
    "#\n",
    "for i,tt in enumerate(obstime):\n",
    "    pred    = prediction(tt,res.x)\n",
    "    rr      = np.sqrt(np.sum(pred**2))\n",
    "    print(\"tobs[{:d}]={:10.5f} vs predicted {:10.5f}\".format(i,tobs[i],pred[1]/rr))\n",
    "    print(\"pobs[{:d}]={:10.5f} vs predicted {:10.5f}\".format(i,pobs[i],pred[2]/rr))\n",
    "# and we can actually look at the underlying 3D positions as well.\n",
    "print(\"\\n\")\n",
    "for i,tt in enumerate(obstime):\n",
    "    truth   = prediction(tt,alpha_true)\n",
    "    pred    = prediction(tt,res.x)\n",
    "    #\n",
    "    truth   = [\"{:10.5f}\".format(x) for x in truth]\n",
    "    pred    = [\"{:10.5f}\".format(x) for x in pred]\n",
    "    #\n",
    "    outstr  = \"x({:6.2f})=\".format(tt)\n",
    "    for itm in truth: outstr += itm\n",
    "    outstr += \" vs predicted \"\n",
    "    for itm in pred: outstr += itm\n",
    "    print(outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many function evaluations did the minimizer take when it was able to make use of gradient information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other references ###\n",
    "\n",
    "Multigrid article\n",
    "https://link.springer.com/article/10.1007/BF01227487\n",
    "\n",
    "NASA report.\n",
    "https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19700016051.pdf\n",
    "\n",
    "Kalman filter & numerical integration.\n",
    "https://www.degruyter.com/downloadpdf/j/arsa.2014.49.issue-2/arsa-2014-0007/arsa-2014-0007.pdf\n",
    "\n",
    "Quasi-linearization\n",
    "http://www.dtic.mil/dtic/tr/fulltext/u2/608287.pdf\n",
    "\n",
    "Taylor expansion\n",
    "http://lfvn.astronomer.ru/report/0000037/timflohrer_2.pdf\n",
    "\n",
    "Review article\n",
    "http://www.jhuapl.edu/techdigest/TD/td2703/vetter.pdf\n",
    "\n",
    "Discussion of classical methods and optimization [hard to follow]\n",
    "https://www.hindawi.com/journals/aaa/2013/960582/\n",
    "\n",
    "Ideas about the \"f and g\" methods\n",
    "https://academic.oup.com/mnras/article/391/3/1259/978116\n",
    "\n",
    "Lectures notes on Laplace and Gauss methods\n",
    "https://www.stardust2013.eu/Portals/63/Images/Training/OTS%20Repository/gronchi_OTS2013.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
